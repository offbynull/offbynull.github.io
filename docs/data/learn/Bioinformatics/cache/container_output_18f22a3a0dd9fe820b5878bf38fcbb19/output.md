<div style="margin:2em; background-color: #e0e0e0;">

<strong>⚠️NOTE️️️⚠️</strong>

I didn't cover it here, but the book dedicated a very large number of sections to introducing this algorithm using a "biased coin" flipping scenario. In the scenario, some guy has two coins, each with a different bias for turning up heads (coinA with biasA / coinB with biasB). At every 10 flip interval, he picks one of the coins at random (either keeps existing one or exchanges it) before using that coin to do another 10 flips.

Which coin he picks per 10 flip round and the coin biases are secret (you don't know them). The only information you have is the outcome of each 10 flip round. Your job is to guess the coin biases from observing those 10 flip rounds, not knowing which of the two coins were used per round.

In this scenario ...

 * TRUE CENTERS = What biasA and biasB actually are.

   (e.g. coinA has a 0.7 bias to turn up heads / coinB has a 0.2 bias to turn up heads)

 * ESTIMATED CENTERS = What you _estimate_ biasA and biasB are (not what they actually are).

   (e.g. coinA has a 0.67 bias to turn up heads / coinB has a 0.23 bias to turn up heads)

 * POINTS = Each 10 flip round's percentage that turned up heads.

   (e.g. HHTHHTHHTT = 6 / 10 = 0.6)

 * CONFIDENCE VALUES = Each 10 flip round's confidence that biasA vs biasB was used (_estimated_ biases).

   (e.g. for round1, 0.1 probability that _estimated_ biasA was used vs 0.9 probability that _estimated_ biasB was used)

In this scenario, the guy does 5 rounds of 10 coin flips (which coin he used per round is a secret). These rounds are your 1-dimensional POINTS ...

```
HTTTHTTHTH = 4 / 10 = 0.4
HHHHTHHHHH = 9 / 10 = 0.9
HTHHHHHTHH = 9 / 10 = 0.8
HTTTTTHHTT = 3 / 10 = 0.3
THHHTHHHTH = 7 / 10 = 0.7
```

You start off by picking two of these percentages as your guess for biasA and biasB (ESTIMATED CENTERS)...

```
biasA = 0.3, biasB = 0.8
```

From there, you're looping over the E-step and M-step...

 * E-step: For each 10 flip round (POINT) and estimated coin bias (ESTIMATED CENTER) pair, calculate the probability (CONFIDENCE VALUE) that that bias generated those 10 flips.
 * M-step: Adjust your estimate for biasA and biasB (ESTIMATED CENTERS) based on those probabilities (CONFIDENCE VALUES).
 
Note that you never actually know what the real coin biases (TRUE CENTERS) are, but you should get somewhere close given that ...

1. there are enough 10 flip rounds (POINTS),
2. you make a decent starting guess for biasA and biasB (initial ESTIMATED CENTERS),
3. and the metric you're using to derive coin usage probabilities (CONFIDENCE VALUES) make sense -- in this scenario it wasn't the partition function but the "conditional probability" that the 10 flip outcome was generated by a coin with the guessed bias.

This scenario was difficult to wrap my head around because the explanations were obtuse and it doesn't make one key concept explicit: The heads average for a 10 flip round (POINT) is representative of the actual heads bias of the coin used (TRUE CENTER). For example, if the coin being used has an actual heads bias of 0.7 (TRUE CENTER of 0.7), most of its 10 flip rounds will have a heads percentage of around 0.7 (POINTS near 0.7). A few might not, but most will (e.g. there's a small chance that the 10 flips could come out to all tails).

If you think about it, this is exactly what's happening with clustering: The points in a cluster are representative of some ideal center point, and you're trying to find what that center point is.

Other things that made the coin flipping example not good:

 1. There's absolutely a chance that the algorithm will veer towards guesses that are far off than true coin biases (an example of this happening would have been helpful).
 2. It should have somehow been emphasized that poor initial coin bias guesses or weak / little coin flip representations can screw you (an example of this happening would have been helpful).
 3. The mathy way in which things were described made everything incredibly obtuse: obtuse naming (e.g. "hidden matrix"), vector notation, dot product, formula representations, etc.. (better naming and more text representation would have made things easier to understand).
 4. Some mathy portions were just papered over (e.g. it was never explained in layman's terms what the partition function actually does -- does it mimic gravity?).

Points 1 and 2 have similar analogs in Lloyd's algorithm. Lloyd's algorithm can give you bad centers + Lloyd's algorithm can screw you if your initial centers are bad / not enough points representative of actual clusters are available.
</div>

